{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nRemoving correlated variables\n=============================\n\nIn this example we show how to remove correlated categorical variables.\n\n1. Strict determinism\n---------------------\n\nLet's consider the following dataset:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n\ndf = pd.DataFrame({\n   'U': [\"a\", \"b\", \"d\", \"a\", \"b\", \"c\", \"a\", \"b\", \"d\", \"c\"],\n   'V': [\"a\", \"b\", \"c\", \"a\", \"b\", \"c\", \"a\", \"b\", \"c\", \"c\"],\n   'W': [\"a\", \"b\", \"b\", \"a\", \"b\", \"b\", \"a\", \"b\", \"b\", \"b\"],\n   'X': [\"a\", \"a\", \"b\", \"b\", \"a\", \"c\", \"c\", \"a\", \"b\", \"c\"],\n   'Y': [\"b\", \"b\", \"c\", \"c\", \"b\", \"a\", \"a\", \"b\", \"c\", \"a\"],\n   'Z': [\"a\", \"a\", \"b\", \"a\", \"a\", \"b\", \"b\", \"a\", \"a\", \"b\"]\n})\nprint(\"Columns in df: %s\" % list(df.columns))\ndf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can detect correlated categorical variables (functional dependencies):\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from qdscreen import qd_screen\n\n# detect strict deterministic relationships\nqd_forest = qd_screen(df)\nprint(qd_forest)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So with only features `U`, and `X` we should be able to predict `V`, `W`, and `Y`. `Z` is a root but has no children\nso it does not help.\n\nWe can create a feature selection model from this deterministic forest object:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "feat_selector = qd_forest.fit_selector_model(df)\nfeat_selector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This model can be used to preprocess the dataset before a learning task:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "only_important_features_df = feat_selector.remove_qd(df)\nonly_important_features_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It can also be used to restore the dependent columns from the remaining ones:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "restored_full_df = feat_selector.predict_qd(only_important_features_df)\nrestored_full_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that the order of columns differs from origin, but apart from this,\nthe restored dataframe is the same as the original:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pd.testing.assert_frame_equal(df, restored_full_df[df.columns])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. Quasi determinism\n---------------------\n\nIn the above example, we used the default settings for `qd_screen`. By default only deterministic relationships are\ndetected, which means that only variables that can perfectly be predicted (without loss of information) from others\nin the dataset are removed.\n\nIn real-world datasets, some noise can occur in the data, or some very rare cases might happen, that you may wish to\ndiscard. Let's first look at the strength of the various relationships thanks to `keep_stats=True`:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# same than above, but this time remember the various indicators\nqd_forest = qd_screen(df, keep_stats=True)\n\n# display them\nprint(qd_forest.stats)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the last row of the last table (relative conditional entropies) we see that variable `Z`'s entropies decreases\ndrastically to reach 28% of its initial entropy, if `X` or `Y` is known. So if we use quasi-determinism with relative\nthreshold of 29% `Z` would be eliminated.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# detect quasi deterministic relationships\nqd_forest2 = qd_screen(df, relative_eps=0.29)\nprint(qd_forest2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This time `Z` is correctly determined as being predictible from `X`.\n\n!!! note \"equivalent nodes\"\n    `X` and `Y` are equivalent variables so each of them could be the parent of the other. To avoid cycles so that the\n    result is still a forest (a set of trees), `X` was arbitrary selected as being the \"representative\" parent of all\n    its equivalents, and `Z` is attached to this representative parent.\n\nAnother, easier way to detect that setting a relative threshold to 29% would eliminate `Z` is to print the\nconditional entropies in increasing order:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ce_df = qd_forest.get_entropies_table(from_to=False, sort_by=\"rel_cond_entropy\")\nce_df.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Or to use the helper plot function:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "qd_forest.plot_increasing_entropies()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3. Integrating with scikit-learn\n--------------------------------\n\n`scikit-learn` is one of the most popular machine learning frameworks in python. It comes with a concept of\n`Pipeline` allowing you to chain several operators to make a model. `qdscreen` provides a `QDScreen` class for easy\nintegration. It works exactly like other feature selection models in scikit-learn (e.g.\n[`VarianceThreshold`](https://scikit-learn.org/stable/modules/feature_selection.html#variance-threshold)):\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from qdscreen.sklearn import QDScreen\n\nX = [[0, 2, 0, 3],\n     [0, 1, 4, 3],\n     [0, 1, 1, 3]]\n\nselector = QDScreen()\nXsel = selector.fit_transform(X)\nXsel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "selector.inverse_transform(Xsel)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}