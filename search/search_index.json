{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"qdscreen \u00b6 Remove redundancy in your categorical variables and increase your models performance. qdscreen provides a python implementation of the Quasi-determinism screening algorithm (also known as qds-BNSL ) from T.Rahier's PhD thesis, 2018. Most data scientists are familiar with the concept of correlation between continuous variables. This concept extends to categorical variables, and is known as functional dependency in the field of relational databases mining. We also name it determinism in the context of Machine Learning and Statistics, to indicate that when a random variable X is known then the value of another variable Y is determined with absolute certainty. \"Quasi-\" determinism is an extension of this concept to handle noise or extremely rare cases in data. qdscreen is able to detect and remove (quasi-)deterministic relationships in a dataset: either as a preprocessing step in any general-purpose data science pipeline or as an accelerator of a Bayesian Network Structure Learning method such as pyGOBN Installing \u00b6 > pip install qdscreen Usage \u00b6 1. Remove correlated variables \u00b6 See this example . 2. Learn a Bayesian Network structure \u00b6 TODO see #6 . Main features / benefits \u00b6 A feature selection algorithm able to eliminate quasi-deterministic relationships a base version compliant with numpy and pandas datasets a scikit-learn compliant version (numpy only) An accelerator for Bayesian Network Structure Learning tasks See Also \u00b6 Bayesian Network libraries in python: pyGOBN (MIT license) pgmpy (MIT license) pomegranate (MIT license) bayespy (MIT license) Functional dependencies libraries in python: fd_miner , an algorithm that was used in this paper . The repository contains a list of reference datasets too. FDTool a python 2 algorithm to mine for functional dependencies, equivalences and candidate keys. From this paper . functional-dependencies functional-dependency-finder connects to a MySQL db and finds functional dependencies. Other libs for probabilistic inference: pyjags (GPLv2 license) edward (Apache License, Version 2.0) Stackoverflow discussions: detecting normal forms canonical cover Others \u00b6 Do you like this library ? You might also like smarie's other python libraries Want to contribute ? \u00b6 Details on the github page: https://github.com/python-qds/qdscreen","title":"Home"},{"location":"#qdscreen","text":"Remove redundancy in your categorical variables and increase your models performance. qdscreen provides a python implementation of the Quasi-determinism screening algorithm (also known as qds-BNSL ) from T.Rahier's PhD thesis, 2018. Most data scientists are familiar with the concept of correlation between continuous variables. This concept extends to categorical variables, and is known as functional dependency in the field of relational databases mining. We also name it determinism in the context of Machine Learning and Statistics, to indicate that when a random variable X is known then the value of another variable Y is determined with absolute certainty. \"Quasi-\" determinism is an extension of this concept to handle noise or extremely rare cases in data. qdscreen is able to detect and remove (quasi-)deterministic relationships in a dataset: either as a preprocessing step in any general-purpose data science pipeline or as an accelerator of a Bayesian Network Structure Learning method such as pyGOBN","title":"qdscreen"},{"location":"#installing","text":"> pip install qdscreen","title":"Installing"},{"location":"#usage","text":"","title":"Usage"},{"location":"#1-remove-correlated-variables","text":"See this example .","title":"1. Remove correlated variables"},{"location":"#2-learn-a-bayesian-network-structure","text":"TODO see #6 .","title":"2. Learn a Bayesian Network structure"},{"location":"#main-features-benefits","text":"A feature selection algorithm able to eliminate quasi-deterministic relationships a base version compliant with numpy and pandas datasets a scikit-learn compliant version (numpy only) An accelerator for Bayesian Network Structure Learning tasks","title":"Main features / benefits"},{"location":"#see-also","text":"Bayesian Network libraries in python: pyGOBN (MIT license) pgmpy (MIT license) pomegranate (MIT license) bayespy (MIT license) Functional dependencies libraries in python: fd_miner , an algorithm that was used in this paper . The repository contains a list of reference datasets too. FDTool a python 2 algorithm to mine for functional dependencies, equivalences and candidate keys. From this paper . functional-dependencies functional-dependency-finder connects to a MySQL db and finds functional dependencies. Other libs for probabilistic inference: pyjags (GPLv2 license) edward (Apache License, Version 2.0) Stackoverflow discussions: detecting normal forms canonical cover","title":"See Also"},{"location":"#others","text":"Do you like this library ? You might also like smarie's other python libraries","title":"Others"},{"location":"#want-to-contribute","text":"Details on the github page: https://github.com/python-qds/qdscreen","title":"Want to contribute ?"},{"location":"api_reference/","text":"API reference \u00b6 In general, using help(symbol) is the recommended way to get the latest documentation. In addition, this page provides an overview of the various elements in this package. Main symbols \u00b6 qd_screen \u00b6 def qd_screen ( X : Union [ pd . DataFrame , np . ndarray ], absolute_eps : float = None , relative_eps : float = None , keep_stats : bool = False ) -> QDForest Finds the (quasi-)deterministic relationships (functional dependencies) between the variables in X , and returns a QDForest object representing the forest of (quasi-)deterministic trees. This object can then be used to fit a feature selection model or to learn a Bayesian Network structure. By default only deterministic relationships are detected. Quasi-determinism can be enabled by setting either an threshold on conditional entropies ( absolute_eps ) or on relative conditional entropies ( relative_eps ). Only one of them should be set. By default ( keep_stats=False ) the entropies tables are not preserved once the forest has been created. If you wish to keep them available, set keep_stats=True . The entropies tables are then available in the <QDForest>.stats attribute, and threshold analysis methods such as <QDForest>.get_entropies_table(...) and <QDForest>.plot_increasing_entropies() become available. Parameters: X : the dataset as a pandas DataFrame or a numpy array. Columns represent the features to compare. absolute_eps : Absolute entropy threshold. Any feature Y that can be predicted from another feature X in a quasi-deterministic way, that is, where conditional entropy H(Y|X) <= absolute_eps , will be removed. The default value is 0 and corresponds to removing deterministic relationships only. relative_eps : Relative entropy threshold. Any feature Y that can be predicted from another feature X in a quasi-deterministic way, that is, where relative conditional entropy H(Y|X)/H(Y) <= relative_eps (a value between 0 and 1 ), will be removed. Only one of absolute_eps or relative_eps should be provided. keep_stats : a boolean indicating if the various entropies tables computed in the process should be kept in memory in the resulting forest object ( <QDForest>.stats ), for further analysis. By default this is False . QDForest \u00b6 TODO QDSelectorModel \u00b6 TODO Entropies \u00b6 TODO scikit-learn symbols \u00b6 QDScreen \u00b6","title":"API reference"},{"location":"api_reference/#api-reference","text":"In general, using help(symbol) is the recommended way to get the latest documentation. In addition, this page provides an overview of the various elements in this package.","title":"API reference"},{"location":"api_reference/#main-symbols","text":"","title":"Main symbols"},{"location":"api_reference/#qd_screen","text":"def qd_screen ( X : Union [ pd . DataFrame , np . ndarray ], absolute_eps : float = None , relative_eps : float = None , keep_stats : bool = False ) -> QDForest Finds the (quasi-)deterministic relationships (functional dependencies) between the variables in X , and returns a QDForest object representing the forest of (quasi-)deterministic trees. This object can then be used to fit a feature selection model or to learn a Bayesian Network structure. By default only deterministic relationships are detected. Quasi-determinism can be enabled by setting either an threshold on conditional entropies ( absolute_eps ) or on relative conditional entropies ( relative_eps ). Only one of them should be set. By default ( keep_stats=False ) the entropies tables are not preserved once the forest has been created. If you wish to keep them available, set keep_stats=True . The entropies tables are then available in the <QDForest>.stats attribute, and threshold analysis methods such as <QDForest>.get_entropies_table(...) and <QDForest>.plot_increasing_entropies() become available. Parameters: X : the dataset as a pandas DataFrame or a numpy array. Columns represent the features to compare. absolute_eps : Absolute entropy threshold. Any feature Y that can be predicted from another feature X in a quasi-deterministic way, that is, where conditional entropy H(Y|X) <= absolute_eps , will be removed. The default value is 0 and corresponds to removing deterministic relationships only. relative_eps : Relative entropy threshold. Any feature Y that can be predicted from another feature X in a quasi-deterministic way, that is, where relative conditional entropy H(Y|X)/H(Y) <= relative_eps (a value between 0 and 1 ), will be removed. Only one of absolute_eps or relative_eps should be provided. keep_stats : a boolean indicating if the various entropies tables computed in the process should be kept in memory in the resulting forest object ( <QDForest>.stats ), for further analysis. By default this is False .","title":"qd_screen"},{"location":"api_reference/#qdforest","text":"TODO","title":"QDForest"},{"location":"api_reference/#qdselectormodel","text":"TODO","title":"QDSelectorModel"},{"location":"api_reference/#entropies","text":"TODO","title":"Entropies"},{"location":"api_reference/#scikit-learn-symbols","text":"","title":"scikit-learn symbols"},{"location":"api_reference/#qdscreen","text":"","title":"QDScreen"},{"location":"changelog/","text":"Changelog \u00b6 0.6.2 - Warning filter \u00b6 Now filtering UserWarning in fit_selector_model even in the sklearn adapter. Fixes #20 0.6.1 - Minor changes \u00b6 Now filtering UserWarning in fit_selector_model . See #20 New build system: now using virtualenv instead of conda in nox sessions. Fixes #23 New project layout to avoid bug with xunitparser . Fixes #18 0.6.0 - sklearn api renames \u00b6 Migrated CI/CD from Travis to Github Actions + nox . selector_skl module renamed sklearn and QDSSelector renamed QDScreen . Fixes #16 0.5.0 - First public working release \u00b6 Initial release with: A main method qd_screen to get the (adjacency matrix of) the quasi-deterministic-forest, a QDForest object with string representation of arcs (Fixes #8 ). Possibility to keep_stats so as to analyse the (conditional) entropies in order to define a \"good\" threshold. A method <QDForest>.fit_selector_model(X) to fit a QDSelectorModel feature selection model able to select relevant features and to predict missing ones. Fixes #7 Support for both pandas dataframes and numpy arrays as input. Fixes #2 A Scikit-learn compliant feature selector QDSSelector , providing the exact same functionality as above but compliant with scikit-learn Pipeline s. Fixes #1 Non-functional: Travis continuous integration, generating documentation and deploying releases on PyPi A package level __version__ attribute. Fixes #3 Added py.typed for PEP561 compliance. Fixed #4 Initial setup.py and setup.cfg","title":"Changelog"},{"location":"changelog/#changelog","text":"","title":"Changelog"},{"location":"changelog/#062-warning-filter","text":"Now filtering UserWarning in fit_selector_model even in the sklearn adapter. Fixes #20","title":"0.6.2 - Warning filter"},{"location":"changelog/#061-minor-changes","text":"Now filtering UserWarning in fit_selector_model . See #20 New build system: now using virtualenv instead of conda in nox sessions. Fixes #23 New project layout to avoid bug with xunitparser . Fixes #18","title":"0.6.1 - Minor changes"},{"location":"changelog/#060-sklearn-api-renames","text":"Migrated CI/CD from Travis to Github Actions + nox . selector_skl module renamed sklearn and QDSSelector renamed QDScreen . Fixes #16","title":"0.6.0 - sklearn api renames"},{"location":"changelog/#050-first-public-working-release","text":"Initial release with: A main method qd_screen to get the (adjacency matrix of) the quasi-deterministic-forest, a QDForest object with string representation of arcs (Fixes #8 ). Possibility to keep_stats so as to analyse the (conditional) entropies in order to define a \"good\" threshold. A method <QDForest>.fit_selector_model(X) to fit a QDSelectorModel feature selection model able to select relevant features and to predict missing ones. Fixes #7 Support for both pandas dataframes and numpy arrays as input. Fixes #2 A Scikit-learn compliant feature selector QDSSelector , providing the exact same functionality as above but compliant with scikit-learn Pipeline s. Fixes #1 Non-functional: Travis continuous integration, generating documentation and deploying releases on PyPi A package level __version__ attribute. Fixes #3 Added py.typed for PEP561 compliance. Fixed #4 Initial setup.py and setup.cfg","title":"0.5.0 - First public working release"},{"location":"long_description/","text":"qdscreen \u00b6 Remove redundancy in your categorical variables and increase your models performance. qdscreen is a python implementation of the Quasi-determinism screening algorithm from T.Rahier's PhD thesis, 2018. The documentation for users is available here: https://python-qds.github.io/qdscreen/ A readme for developers is available here: https://github.com/python-qds/qdscreen","title":"qdscreen"},{"location":"long_description/#qdscreen","text":"Remove redundancy in your categorical variables and increase your models performance. qdscreen is a python implementation of the Quasi-determinism screening algorithm from T.Rahier's PhD thesis, 2018. The documentation for users is available here: https://python-qds.github.io/qdscreen/ A readme for developers is available here: https://github.com/python-qds/qdscreen","title":"qdscreen"},{"location":"generated/gallery/","text":"Usage examples \u00b6 These examples demonstrate how to use the library. Removing correlated variables Download all examples in Python source code: gallery_python.zip Download all examples in Jupyter notebooks: gallery_jupyter.zip Gallery generated by mkdocs-gallery","title":"Usage examples"},{"location":"generated/gallery/#usage-examples","text":"These examples demonstrate how to use the library. Removing correlated variables Download all examples in Python source code: gallery_python.zip Download all examples in Jupyter notebooks: gallery_jupyter.zip Gallery generated by mkdocs-gallery","title":"Usage examples"},{"location":"generated/gallery/1_remove_correlated_vars_demo/","text":"Note Click here to download the full example code Removing correlated variables \u00b6 In this example we show how to remove correlated categorical variables. 1. Strict determinism \u00b6 Let's consider the following dataset: import pandas as pd df = pd . DataFrame ({ 'U' : [ \"a\" , \"b\" , \"d\" , \"a\" , \"b\" , \"c\" , \"a\" , \"b\" , \"d\" , \"c\" ], 'V' : [ \"a\" , \"b\" , \"c\" , \"a\" , \"b\" , \"c\" , \"a\" , \"b\" , \"c\" , \"c\" ], 'W' : [ \"a\" , \"b\" , \"b\" , \"a\" , \"b\" , \"b\" , \"a\" , \"b\" , \"b\" , \"b\" ], 'X' : [ \"a\" , \"a\" , \"b\" , \"b\" , \"a\" , \"c\" , \"c\" , \"a\" , \"b\" , \"c\" ], 'Y' : [ \"b\" , \"b\" , \"c\" , \"c\" , \"b\" , \"a\" , \"a\" , \"b\" , \"c\" , \"a\" ], 'Z' : [ \"a\" , \"a\" , \"b\" , \"a\" , \"a\" , \"b\" , \"b\" , \"a\" , \"a\" , \"b\" ] }) print ( \"Columns in df: %s \" % list ( df . columns )) df Out: Columns in df: [ 'U' , 'V' , 'W' , 'X' , 'Y' , 'Z' ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } U V W X Y Z 0 a a a a b a 1 b b b a b a 2 d c b b c b 3 a a a b c a 4 b b b a b a 5 c c b c a b 6 a a a c a b 7 b b b a b a 8 d c b b c a 9 c c b c a b We can detect correlated categorical variables (functional dependencies): from qdscreen import qd_screen # detect strict deterministic relationships qd_forest = qd_screen ( df ) print ( qd_forest ) Out: QDForest ( 6 vars ) : - 3 roots ( 1 +2* ) : U*, X*, Z - 3 other nodes: V, W, Y U \u2514\u2500 V \u2514\u2500 W X \u2514\u2500 Y So with only features U , and X we should be able to predict V , W , and Y . Z is a root but has no children so it does not help. We can create a feature selection model from this deterministic forest object: feat_selector = qd_forest . fit_selector_model ( df ) feat_selector Out: <qdscreen.selector.QDSelectorModel object at 0x7f949630a700> This model can be used to preprocess the dataset before a learning task: only_important_features_df = feat_selector . remove_qd ( df ) only_important_features_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } U X Z 0 a a a 1 b a a 2 d b b 3 a b a 4 b a a 5 c c b 6 a c b 7 b a a 8 d b a 9 c c b It can also be used to restore the dependent columns from the remaining ones: restored_full_df = feat_selector . predict_qd ( only_important_features_df ) restored_full_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } U X Z V W Y 0 a a a a a b 1 b a a b b b 2 d b b c b c 3 a b a a a c 4 b a a b b b 5 c c b c b a 6 a c b a a a 7 b a a b b b 8 d b a c b c 9 c c b c b a Note that the order of columns differs from origin, but apart from this, the restored dataframe is the same as the original: pd . testing . assert_frame_equal ( df , restored_full_df [ df . columns ]) 2. Quasi determinism \u00b6 In the above example, we used the default settings for qd_screen . By default only deterministic relationships are detected, which means that only variables that can perfectly be predicted (without loss of information) from others in the dataset are removed. In real-world datasets, some noise can occur in the data, or some very rare cases might happen, that you may wish to discard. Let's first look at the strength of the various relationships thanks to keep_stats=True : # same than above, but this time remember the various indicators qd_forest = qd_screen ( df , keep_stats = True ) # display them print ( qd_forest . stats ) Out: Statistics computed for dataset: U V W X Y Z 0 a a a a b a 1 b b b a b a ... ( 10 rows ) Entropies ( H ) : U 1 .970951 V 1 .570951 W 0 .881291 X 1 .570951 Y 1 .570951 Z 0 .970951 dtype: float64 Conditional entropies ( Hcond = H ( row | col )) : U V W X Y Z U 0 .000000 0 .400000 1 .089660 0 .875489 0 .875489 1 .475489 V 0 .000000 0 .000000 0 .689660 0 .875489 0 .875489 1 .200000 W 0 .000000 0 .000000 0 .000000 0 .875489 0 .875489 0 .875489 X 0 .475489 0 .875489 1 .565148 0 .000000 0 .000000 0 .875489 Y 0 .475489 0 .875489 1 .565148 0 .000000 0 .000000 0 .875489 Z 0 .475489 0 .600000 0 .965148 0 .275489 0 .275489 0 .000000 Relative conditional entropies ( Hcond_rel = H ( row | col ) /H ( row )) : U V W X Y Z U 0 .000000 0 .202948 0 .552860 0 .444196 0 .444196 0 .748618 V 0 .000000 0 .000000 0 .439008 0 .557299 0 .557299 0 .763869 W 0 .000000 0 .000000 0 .000000 0 .993416 0 .993416 0 .993416 X 0 .302676 0 .557299 0 .996307 0 .000000 0 .000000 0 .557299 Y 0 .302676 0 .557299 0 .996307 0 .000000 0 .000000 0 .557299 Z 0 .489715 0 .617951 0 .994024 0 .283731 0 .283731 0 .000000 In the last row of the last table (relative conditional entropies) we see that variable Z 's entropies decreases drastically to reach 28% of its initial entropy, if X or Y is known. So if we use quasi-determinism with relative threshold of 29% Z would be eliminated. # detect quasi deterministic relationships qd_forest2 = qd_screen ( df , relative_eps = 0.29 ) print ( qd_forest2 ) Out: QDForest ( 6 vars ) : - 2 roots ( 0 +2* ) : U*, X* - 4 other nodes: V, W, Y, Z U \u2514\u2500 V \u2514\u2500 W X \u2514\u2500 Y \u2514\u2500 Z This time Z is correctly determined as being predictible from X . equivalent nodes X and Y are equivalent variables so each of them could be the parent of the other. To avoid cycles so that the result is still a forest (a set of trees), X was arbitrary selected as being the \"representative\" parent of all its equivalents, and Z is attached to this representative parent. Another, easier way to detect that setting a relative threshold to 29% would eliminate Z is to print the conditional entropies in increasing order: ce_df = qd_forest . get_entropies_table ( from_to = False , sort_by = \"rel_cond_entropy\" ) ce_df . head ( 10 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } cond_entropy rel_cond_entropy arc U->V 0.000000 0.000000 U->W 0.000000 0.000000 V->W 0.000000 0.000000 Y->X 0.000000 0.000000 X->Y 0.000000 0.000000 V->U 0.400000 0.202948 Y->Z 0.275489 0.283731 X->Z 0.275489 0.283731 U->X 0.475489 0.302676 U->Y 0.475489 0.302676 Or to use the helper plot function: qd_forest . plot_increasing_entropies () 3. Integrating with scikit-learn \u00b6 scikit-learn is one of the most popular machine learning frameworks in python. It comes with a concept of Pipeline allowing you to chain several operators to make a model. qdscreen provides a QDScreen class for easy integration. It works exactly like other feature selection models in scikit-learn (e.g. VarianceThreshold ): from qdscreen.sklearn import QDScreen X = [[ 0 , 2 , 0 , 3 ], [ 0 , 1 , 4 , 3 ], [ 0 , 1 , 1 , 3 ]] selector = QDScreen () Xsel = selector . fit_transform ( X ) Xsel Out: array ([[ 0 ] , [ 4 ] , [ 1 ]]) selector . inverse_transform ( Xsel ) Out: array ([[ 0 , 2 , 0 , 3 ] , [ 0 , 1 , 4 , 3 ] , [ 0 , 1 , 1 , 3 ]]) Total running time of the script: ( 0 minutes 0.856 seconds) Download Python source code: 1_remove_correlated_vars_demo.py Download Jupyter notebook: 1_remove_correlated_vars_demo.ipynb Gallery generated by mkdocs-gallery","title":"Removing correlated variables"},{"location":"generated/gallery/1_remove_correlated_vars_demo/#removing-correlated-variables","text":"In this example we show how to remove correlated categorical variables.","title":"Removing correlated variables"},{"location":"generated/gallery/1_remove_correlated_vars_demo/#1-strict-determinism","text":"Let's consider the following dataset: import pandas as pd df = pd . DataFrame ({ 'U' : [ \"a\" , \"b\" , \"d\" , \"a\" , \"b\" , \"c\" , \"a\" , \"b\" , \"d\" , \"c\" ], 'V' : [ \"a\" , \"b\" , \"c\" , \"a\" , \"b\" , \"c\" , \"a\" , \"b\" , \"c\" , \"c\" ], 'W' : [ \"a\" , \"b\" , \"b\" , \"a\" , \"b\" , \"b\" , \"a\" , \"b\" , \"b\" , \"b\" ], 'X' : [ \"a\" , \"a\" , \"b\" , \"b\" , \"a\" , \"c\" , \"c\" , \"a\" , \"b\" , \"c\" ], 'Y' : [ \"b\" , \"b\" , \"c\" , \"c\" , \"b\" , \"a\" , \"a\" , \"b\" , \"c\" , \"a\" ], 'Z' : [ \"a\" , \"a\" , \"b\" , \"a\" , \"a\" , \"b\" , \"b\" , \"a\" , \"a\" , \"b\" ] }) print ( \"Columns in df: %s \" % list ( df . columns )) df Out: Columns in df: [ 'U' , 'V' , 'W' , 'X' , 'Y' , 'Z' ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } U V W X Y Z 0 a a a a b a 1 b b b a b a 2 d c b b c b 3 a a a b c a 4 b b b a b a 5 c c b c a b 6 a a a c a b 7 b b b a b a 8 d c b b c a 9 c c b c a b We can detect correlated categorical variables (functional dependencies): from qdscreen import qd_screen # detect strict deterministic relationships qd_forest = qd_screen ( df ) print ( qd_forest ) Out: QDForest ( 6 vars ) : - 3 roots ( 1 +2* ) : U*, X*, Z - 3 other nodes: V, W, Y U \u2514\u2500 V \u2514\u2500 W X \u2514\u2500 Y So with only features U , and X we should be able to predict V , W , and Y . Z is a root but has no children so it does not help. We can create a feature selection model from this deterministic forest object: feat_selector = qd_forest . fit_selector_model ( df ) feat_selector Out: <qdscreen.selector.QDSelectorModel object at 0x7f949630a700> This model can be used to preprocess the dataset before a learning task: only_important_features_df = feat_selector . remove_qd ( df ) only_important_features_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } U X Z 0 a a a 1 b a a 2 d b b 3 a b a 4 b a a 5 c c b 6 a c b 7 b a a 8 d b a 9 c c b It can also be used to restore the dependent columns from the remaining ones: restored_full_df = feat_selector . predict_qd ( only_important_features_df ) restored_full_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } U X Z V W Y 0 a a a a a b 1 b a a b b b 2 d b b c b c 3 a b a a a c 4 b a a b b b 5 c c b c b a 6 a c b a a a 7 b a a b b b 8 d b a c b c 9 c c b c b a Note that the order of columns differs from origin, but apart from this, the restored dataframe is the same as the original: pd . testing . assert_frame_equal ( df , restored_full_df [ df . columns ])","title":"1. Strict determinism"},{"location":"generated/gallery/1_remove_correlated_vars_demo/#2-quasi-determinism","text":"In the above example, we used the default settings for qd_screen . By default only deterministic relationships are detected, which means that only variables that can perfectly be predicted (without loss of information) from others in the dataset are removed. In real-world datasets, some noise can occur in the data, or some very rare cases might happen, that you may wish to discard. Let's first look at the strength of the various relationships thanks to keep_stats=True : # same than above, but this time remember the various indicators qd_forest = qd_screen ( df , keep_stats = True ) # display them print ( qd_forest . stats ) Out: Statistics computed for dataset: U V W X Y Z 0 a a a a b a 1 b b b a b a ... ( 10 rows ) Entropies ( H ) : U 1 .970951 V 1 .570951 W 0 .881291 X 1 .570951 Y 1 .570951 Z 0 .970951 dtype: float64 Conditional entropies ( Hcond = H ( row | col )) : U V W X Y Z U 0 .000000 0 .400000 1 .089660 0 .875489 0 .875489 1 .475489 V 0 .000000 0 .000000 0 .689660 0 .875489 0 .875489 1 .200000 W 0 .000000 0 .000000 0 .000000 0 .875489 0 .875489 0 .875489 X 0 .475489 0 .875489 1 .565148 0 .000000 0 .000000 0 .875489 Y 0 .475489 0 .875489 1 .565148 0 .000000 0 .000000 0 .875489 Z 0 .475489 0 .600000 0 .965148 0 .275489 0 .275489 0 .000000 Relative conditional entropies ( Hcond_rel = H ( row | col ) /H ( row )) : U V W X Y Z U 0 .000000 0 .202948 0 .552860 0 .444196 0 .444196 0 .748618 V 0 .000000 0 .000000 0 .439008 0 .557299 0 .557299 0 .763869 W 0 .000000 0 .000000 0 .000000 0 .993416 0 .993416 0 .993416 X 0 .302676 0 .557299 0 .996307 0 .000000 0 .000000 0 .557299 Y 0 .302676 0 .557299 0 .996307 0 .000000 0 .000000 0 .557299 Z 0 .489715 0 .617951 0 .994024 0 .283731 0 .283731 0 .000000 In the last row of the last table (relative conditional entropies) we see that variable Z 's entropies decreases drastically to reach 28% of its initial entropy, if X or Y is known. So if we use quasi-determinism with relative threshold of 29% Z would be eliminated. # detect quasi deterministic relationships qd_forest2 = qd_screen ( df , relative_eps = 0.29 ) print ( qd_forest2 ) Out: QDForest ( 6 vars ) : - 2 roots ( 0 +2* ) : U*, X* - 4 other nodes: V, W, Y, Z U \u2514\u2500 V \u2514\u2500 W X \u2514\u2500 Y \u2514\u2500 Z This time Z is correctly determined as being predictible from X . equivalent nodes X and Y are equivalent variables so each of them could be the parent of the other. To avoid cycles so that the result is still a forest (a set of trees), X was arbitrary selected as being the \"representative\" parent of all its equivalents, and Z is attached to this representative parent. Another, easier way to detect that setting a relative threshold to 29% would eliminate Z is to print the conditional entropies in increasing order: ce_df = qd_forest . get_entropies_table ( from_to = False , sort_by = \"rel_cond_entropy\" ) ce_df . head ( 10 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } cond_entropy rel_cond_entropy arc U->V 0.000000 0.000000 U->W 0.000000 0.000000 V->W 0.000000 0.000000 Y->X 0.000000 0.000000 X->Y 0.000000 0.000000 V->U 0.400000 0.202948 Y->Z 0.275489 0.283731 X->Z 0.275489 0.283731 U->X 0.475489 0.302676 U->Y 0.475489 0.302676 Or to use the helper plot function: qd_forest . plot_increasing_entropies ()","title":"2. Quasi determinism"},{"location":"generated/gallery/1_remove_correlated_vars_demo/#3-integrating-with-scikit-learn","text":"scikit-learn is one of the most popular machine learning frameworks in python. It comes with a concept of Pipeline allowing you to chain several operators to make a model. qdscreen provides a QDScreen class for easy integration. It works exactly like other feature selection models in scikit-learn (e.g. VarianceThreshold ): from qdscreen.sklearn import QDScreen X = [[ 0 , 2 , 0 , 3 ], [ 0 , 1 , 4 , 3 ], [ 0 , 1 , 1 , 3 ]] selector = QDScreen () Xsel = selector . fit_transform ( X ) Xsel Out: array ([[ 0 ] , [ 4 ] , [ 1 ]]) selector . inverse_transform ( Xsel ) Out: array ([[ 0 , 2 , 0 , 3 ] , [ 0 , 1 , 4 , 3 ] , [ 0 , 1 , 1 , 3 ]]) Total running time of the script: ( 0 minutes 0.856 seconds) Download Python source code: 1_remove_correlated_vars_demo.py Download Jupyter notebook: 1_remove_correlated_vars_demo.ipynb Gallery generated by mkdocs-gallery","title":"3. Integrating with scikit-learn"},{"location":"generated/gallery/mg_execution_times/","text":"Computation times \u00b6 00:00.856 total execution time for generated_gallery files: +----------------------------------------------------------------------------------------------------------------------+-----------+--------+ | 1_remove_correlated_vars_demo (docs/examples/1_remove_correlated_vars_demo.py) | 00:00.856 | 0.0 MB | +----------------------------------------------------------------------------------------------------------------------+-----------+--------+","title":"Computation times"},{"location":"generated/gallery/mg_execution_times/#computation-times","text":"00:00.856 total execution time for generated_gallery files: +----------------------------------------------------------------------------------------------------------------------+-----------+--------+ | 1_remove_correlated_vars_demo (docs/examples/1_remove_correlated_vars_demo.py) | 00:00.856 | 0.0 MB | +----------------------------------------------------------------------------------------------------------------------+-----------+--------+","title":"Computation times"}]}